import os
import sys

# Function to Traverse up until you find the project root
def get_project_root(target="Decoding-Human-Activity-Erdos-Summer-2025"): # replace target name with the name of the root directory/repository name
    path = os.getcwd()
    while os.path.basename(path) != target:
        new_path = os.path.dirname(path)
        if new_path == path:
            raise FileNotFoundError(f"Could not find project root '{target}'.")
        path = new_path
    return path


project_root = get_project_root()
directory_path = os.path.join(project_root, "Python Files") 

# Add to Python path
if directory_path not in sys.path:
    sys.path.append(directory_path)

from utils import *  # * import all or specific functions








X_rf = uci_df.drop(columns=['subject','activity']).values
y_rf = uci_df['activity'].values

rf = RandomForestClassifier(n_estimators=200, random_state=0)
rf.fit(X_rf, y_rf)

feat_names = uci_df.columns.drop(['subject','activity'])
importances = rf.feature_importances_
idx_sorted  = np.argsort(importances)[::-1]

top_n = 10
indices = np.argsort(importances)[::-1][:top_n]

top_names  = feat_names[indices]
top_values = importances[indices]

plt.figure(figsize=(8, 5))
plt.barh(range(top_n), top_values[::-1], align='center')
plt.yticks(range(top_n), top_names[::-1])
plt.xlabel('Feature Importance')
plt.title('Top 10 Random Forest Feature Importances')
plt.tight_layout()
plt.show()



topn_list   = np.arange(5, 562, 5)
rf_to_acc   = {}
feat_names  = uci_df.columns.drop(['subject','activity']).to_numpy()
importances = rf.feature_importances_
idx_sorted  = np.argsort(importances)[::-1]

for topn in topn_list:
    # pick the top‐N features
    top_idx      = idx_sorted[:topn]
    top_features = feat_names[top_idx].tolist()

    # grab the *entire* dataset with those features
    X = uci_df[top_features].values
    y = uci_df['activity'].values

    # do Leave‐one‐out 1‐NN via KD‐tree
    nbrs      = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(X)
    _, inds   = nbrs.kneighbors(X)
    nn        = inds[:,1]                    
    acc       = np.mean(y[nn] == y)

    rf_to_acc[topn] = acc


#plot
Ns   = sorted(rf_to_acc.keys())
accs = [rf_to_acc[n] for n in Ns]
plt.figure(figsize=(20,5))
plt.plot(Ns, accs, '-o')
plt.xlabel('# RF-selected features')
plt.xticks(list(range(0,561,5)),rotation = 90)
plt.ylabel('LOO 1-NN Accuracy')
plt.grid(True)
plt.show()














feature_cols = [c for c in uci_df.columns if c not in ('subject','activity')]
X_raw = uci_df[feature_cols].values
y_all = uci_df['activity'].values

# Standardize
scaler = StandardScaler().fit(X_raw)
X_scaled = scaler.transform(X_raw)

pca = PCA().fit(X_scaled)
X_pca_full = pca.transform(X_scaled)  

pc_cols = [f'PC{i}' for i in range(1, X_pca_full.shape[1]+1)]
df_pca = pd.DataFrame(X_pca_full, columns=pc_cols, index=uci_df.index)
df_pca['subject']  = uci_df['subject']
df_pca['activity'] = uci_df['activity']

topk_list = list(range(5, 561, 5))
pc_to_acc  = {}
y_all      = df_pca['activity'].values

for k in topk_list:
    use_cols = pc_cols[:k]
    Xk       = df_pca[use_cols].values
    yk       = y_all

    # build KD‐tree and query 2 neighbors (self + true NN)
    nbrs      = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(Xk)
    _, inds   = nbrs.kneighbors(Xk)
    nn_idx    = inds[:, 1]         # index of nearest *other* sample

    # compute LOO-1NN accuracy
    acc = np.mean(yk[nn_idx] == yk)
    pc_to_acc[k] = acc





# plot
Ks   = sorted(pc_to_acc.keys())
accs = [pc_to_acc[k] for k in Ks]

plt.figure(figsize=(20,5))
plt.plot(Ks, accs, '-o')
plt.xlabel('Number of PCA Components (k)')
plt.ylabel('LOO 1-NN Accuracy')
plt.title('Accuracy vs. # PCA Components')
plt.grid(True)
plt.xticks(Ks, rotation=90)
plt.tight_layout()
plt.show()





feature_cols = [c for c in uci_df.columns if c not in ('subject','activity')]
X_raw = uci_df[feature_cols].values
y_all = uci_df['activity'].values

# Standardize
scaler = StandardScaler().fit(X_raw)
X_scaled = scaler.transform(X_raw)


pca = PCA().fit(X_scaled)
X_pca_full = pca.transform(X_scaled)  


pc_cols = [f'PC{i}' for i in range(1, X_pca_full.shape[1]+1)]
df_pca = pd.DataFrame(X_pca_full, columns=pc_cols, index=uci_df.index)
df_pca['subject']  = uci_df['subject']
df_pca['activity'] = uci_df['activity']

#  Loop over different # of PCs and compute LOO 1-NN accuracy ---
topk_list   = list(range(5, 561, 5))     # e.g. 5,10,…,100 PCs
N           = 1200                       # total subsample size
C           = df_pca['activity'].nunique()
n_per_class = N // C

pc_to_acc = {}
for k in topk_list:
    # take only the first k PCs
    use_cols = pc_cols[:k]
    df_sub   = df_pca[['subject','activity'] + use_cols]
    
    # stratified sample equal per activity
    df_strat = (
        df_sub
          .groupby('activity', group_keys=False)
          .sample(n=n_per_class, random_state=42)
          .reset_index(drop=True)
    )
    
    # extract X and y
    Xk = df_strat[use_cols].values
    yk = df_strat['activity'].tolist()
    
    # compute LOO 1-NN accuracy
    acc = loo_euclidean_1nn_accuracy(Xk, yk)
    pc_to_acc[k] = acc

# --- 3) Plot the result ---
Ks   = sorted(pc_to_acc.keys())
accs = [pc_to_acc[k] for k in Ks]

plt.figure(figsize=(20,5))
plt.plot(Ks, accs, marker='o', linewidth=2)
plt.xlabel('Number of PCA Components (k)')
plt.ylabel('LOO 1-NN Accuracy')
plt.title('Accuracy vs. # PCA Components')
plt.grid(True)
plt.xticks(Ks, rotation=90)
plt.tight_layout()
plt.show()



evr = pca.explained_variance_ratio_

plt.figure(figsize=(8,4))
plt.bar(np.arange(1, len(evr)+1), evr, width=0.8)
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.title('PCA Scree Plot')
plt.xlim(0.5, 20.5)          # zoom into the first 20 PCs if you like
plt.xticks(np.arange(1,21))  # show ticks 1–20
plt.tight_layout()
plt.show()

















X = uci_df.drop(columns=['subject','activity']).values
y = uci_df['activity'].values
# Fit LDA on the full data first
lda = LDA(n_components=None)  # fit with max components possible (C-1)
X_lda_full = lda.fit_transform(X, y)

# Prepare df_lda DataFrame including subject, activity and LDA components
lda_cols = [f"LDA_{i+1}" for i in range(X_lda_full.shape[1])]
df_lda = uci_df[['subject','activity']].copy()
for i, col in enumerate(lda_cols):
    df_lda[col] = X_lda_full[:, i]

lda_cols = [c for c in df_lda.columns if c.startswith("LDA_")]
topk_list = list(range(1, 10+1))  # 1 through max LDA dims

lda_to_acc = {}
for k in topk_list:
    # 1) Pick first k LDA dims
    use_cols = lda_cols[:k]
    Xk = df_lda[use_cols].values
    yk = df_lda['activity'].values

    # 2) Build KD‐tree & query 2 neighbors (self + true 1‐NN)
    nbrs     = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(Xk)
    distances, indices = nbrs.kneighbors(Xk)

    # 3) indices[:,0] is self, indices[:,1] is the real NN
    nn_idx = indices[:,1]

    # 4) LOO accuracy = fraction whose neighbor has the same label
    acc = np.mean(yk[nn_idx] == yk)
    lda_to_acc[k] = acc

# --- plot ---
Ks   = list(lda_to_acc.keys())
accs = [lda_to_acc[k] for k in Ks]

plt.figure(figsize=(20,5))
plt.plot(Ks, accs, '-o')
plt.xlabel('Number of LDA Components (k)')
plt.ylabel('LOO 1-NN Accuracy')
plt.title('Full‐Dataset LOO 1-NN on LDA Embedding')
plt.grid(True)
plt.xticks(Ks)
plt.tight_layout()
plt.show()

print(f"Accuracy score with 5 LDA components = {accs[4]}")





fig = px.scatter_3d(
    df_lda,
    x='LDA_1',
    y='LDA_2',
    z='LDA_3',
    color='activity',
    title='3D LDA Projection (first 3 components)',
    labels={
        'LDA_1': 'LDA Component 1',
        'LDA_2': 'LDA Component 2',
        'LDA_3': 'LDA Component 3'
    },
    width=1200,
    height=700
)
fig.update_traces(marker=dict(size=4, opacity=0.8))
fig.show()



nbrs     = NearestNeighbors(n_neighbors=2).fit(X_lda_full)
_, inds  = nbrs.kneighbors(X_lda_full)
nn_index = inds[:,1]               # skip self

y_true = y
y_pred = y[nn_index]

# 2) compute confusion matrix
labels = np.unique(y_true)
cm = confusion_matrix(y_true, y_pred, labels=labels)

# 3) plot with seaborn
df_cm = pd.DataFrame(cm, index=labels, columns=labels)

plt.figure(figsize=(10,8))
sns.heatmap(
    df_cm, 
    annot=True, 
    fmt='d', 
    cmap='Blues', 
    cbar_kws={'label': 'Count'}
)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix: LDA + 1-NN (LOO)')
plt.tight_layout()
plt.savefig(f"lda_confusion_matrix.png",bbox_inches = 'tight')
plt.show()






uci_df





import re
df = uci_df.copy() 

# STEP 1: Automatically get all 3D prefixes
xyz_cols = [col for col in df.columns if re.search(r'-[XYZ]$', col)] 

feature_prefixes = set() # will consist of prefixes (without XYZ) e.g. tBodyAcc-mean() 
for col in xyz_cols:
    match = re.match(r'(.+)-[XYZ]$', col) # does col end with X, Y or Z. 
    if match: # if col ends with X, Y or Z match.group(1) gives the name before the -[XYZ] e.g. tBodyAcc-mean() for tBodyAcc-mean()-X
        feature_prefixes.add(match.group(1)) # add this to the set
feature_prefixes = sorted(feature_prefixes)

axes = ['X', 'Y', 'Z']


# STEP 2: Build a mapping from feature_prefix -> axis -> column name
feature_map = {}
for prefix in feature_prefixes:
    feature_map[prefix] = {ax: f"{prefix}-{ax}" for ax in axes if f"{prefix}-{ax}" in df.columns}
    # If any axis is missing, skip this feature
    if len(feature_map[prefix]) != 3:
        del feature_map[prefix]



# STEP 3: Apply a random permutation row-by-row
shuffled_df = df.copy()
for i in range(len(df)): # for each row, we will randomly permuate x,y and z.
    perm = np.random.permutation(axes)  # e.g. ['Y','X','Z'] or ['Z','Y','X'] etc.
    for prefix, axis_map in feature_map.items():
        # Original values in X,Y,Z order
        orig_values = [df.at[i, axis_map[ax]] for ax in axes]
        # Permuted values, assign to shuffled_df
        for idx, ax in enumerate(axes):
            shuffled_df.at[i, axis_map[ax]] = orig_values[perm.tolist().index(ax)]



# remove any "-X", "-Y" or "-Z" at the end of the column name
shuffled_df.columns = shuffled_df.columns.to_series().replace(
    to_replace=r'-(?:X|Y|Z)$',
    value='',
    regex=True
)


shuffled_df






X_shuffled = shuffled_df.drop(columns=['subject','activity']).values
y_shuffled = shuffled_df['activity'].values
# fit LDA
lda = LDA(n_components= 5)
X_shuffled_lda = lda.fit_transform(X_shuffled, y_shuffled)

nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(X_shuffled_lda)
distances, indices = nbrs.kneighbors(X_shuffled_lda)

# indices[:,0] is self, indices[:,1] is the nearest neighbor
nn = indices[:, 1]
accuracy_shuffled = np.mean(y_shuffled[nn] == y_shuffled)
print(f"1-NN LOO accuracy_shuffled: {accuracy_shuffled:.3%}")










