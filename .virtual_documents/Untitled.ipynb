


import pandas as pd
from collections import defaultdict




import pandas as pd
from collections import Counter

# File paths
base = "./Human Activity Recognition using Smartphones/UCI HAR Dataset/"
X_train_path = base + "train/X_train.txt"
y_train_path = base + "train/y_train.txt"
subject_train_path = base + "train/subject_train.txt"
X_test_path = base + "test/X_test.txt"
y_test_path = base + "test/y_test.txt"
subject_test_path = base + "test/subject_test.txt"
features_path = base + "features.txt"
activity_labels_path = base + "activity_labels.txt"

# Load features with deduplication
features_path = "./Human Activity Recognition using Smartphones/UCI HAR Dataset/features.txt"
features_df = pd.read_csv(features_path, sep=r"\s+", header=None)
raw_features = features_df[1].tolist()

# Deduplicate feature names
counts = defaultdict(int)
features = []
for name in raw_features:
    if counts[name]:
        new_name = f"{name}_{counts[name]}"
    else:
        new_name = name
    features.append(new_name)
    counts[name] += 1

# Load datasets
X_train = pd.read_csv(X_train_path, sep=r'\s+', header=None, names=features)
y_train = pd.read_csv(y_train_path, header=None, names=["activity"])
subject_train = pd.read_csv(subject_train_path, header=None, names=["subject"])
X_test = pd.read_csv(X_test_path, sep=r'\s+', header=None, names=features)
y_test = pd.read_csv(y_test_path, header=None, names=["activity"])
subject_test = pd.read_csv(subject_test_path, header=None, names=["subject"])

# Combine
train_df = pd.concat([subject_train, y_train, X_train], axis=1)
test_df = pd.concat([subject_test, y_test, X_test], axis=1)

# Activity label mapping
activity_labels = pd.read_csv(activity_labels_path, sep=r'\s+', header=None, names=["id", "label"])
activity_map = dict(zip(activity_labels.id, activity_labels.label))
train_df["activity"] = train_df["activity"].map(activity_map)
test_df["activity"] = test_df["activity"].map(activity_map)

print(train_df.head())



train_df








import pandas as pd
import glob
import os



file_path = "./HuGaDB/Data/HuGaDB_v1_bicycling_01_00.txt"

df = pd.read_csv(file_path, sep=r"\s+", engine='python', skiprows=3)



df.shape


df


file_pattern = os.path.join(data_folder, "HuGaDB_v1_*.txt")


file_pattern


a = glob.glob(file_pattern)[0]


a





a = os.path.basename(a)





parts = a.replace(".txt", "").split("_")


parts[-2]+parts[1]


parts[3]



def load_hugadb_to_dataframe(data_folder):
    all_records = []
    # match every HuGaDB_v1_<activity>_<subject>_<trial>.txt
    file_pattern = os.path.join(data_folder, "HuGaDB_v1_*.txt")

    for filepath in glob.glob(file_pattern):
        basename = os.path.basename(filepath)
        parts = basename.replace(".txt", "").split("_")
        # parts = ["HuGaDB", "v1", "activity", "subject_id", "counter"]
        if parts[3] == 'str':
            activity = str(parts[2]+parts+[3]+parts[4])
        else:
            activity = parts[2]
        if activity == "various":
            # skip every file whose activity == "various"
            continue

        participant_id = int(parts[-2])
        
        # now read “filepath” as before…
        # (skiprows=3 so that the 4th line is the header row,
        #  then load all numeric rows)
        df = pd.read_csv(
            filepath,
            sep = r"\s+|\t",
            header = 0,
            skiprows = 3,
            engine = 'python'
        )
        df.insert(0, "subject", participant_id)
        df.insert(1, "activity", activity)
        all_records.append(df)

    return pd.concat(all_records, ignore_index = True) if all_records else pd.DataFrame()











data_folder = '/Users/ecekaracam/Documents/GitHub/Erdos Su25/Decoding Human Activity Erdos Summer 2025/HuGaDB/Data'


df = load_hugadb_to_dataframe(data_folder)
# df = df.sort_values(by = ['subject','activity']).reset_index(drop = True)



df['activity']


df


df.to_csv("hugadb_all.csv",index = False)


df = pd.read_csv("/Users/ecekaracam/Documents/GitHub/Erdos Su25/Decoding Human Activity Erdos Summer 2025/HuGaDB Data Frame/hugadb_all.csv"
               )



